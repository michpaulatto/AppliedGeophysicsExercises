{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_tt_land_topo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMwkm37WYS0dgziu/aa9abR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxokEpijOwr-"
      },
      "source": [
        "# Traveltime tomography exercise\n",
        "Created by Michele Paulatto, Imperial College London\n",
        "\n",
        "https://github.com/michpaulatto/AppliedGeophysicsExercises\n",
        "\n",
        "\n",
        "Version 0.5 - 21.01.2021\n",
        "\n",
        "This exercise is distributed under a [GNU GPL 3.0 license](https://www.gnu.org/licenses/gpl-3.0.en.html). \n",
        "\n",
        "![](https://www.gnu.org/graphics/gplv3-with-text-136x68.png)\n",
        "\n",
        "Developed for [Google Colab](https://colab.research.google.com/)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Introduction\n",
        "This is an exercise to demonstrate traveltime tomography. This environment is a Jupyter notebook (https://jupyter.org/), an open-source application for creating interactive modular code. This exercise is written in the Python programming language. You do not need to be an expert in Python or Jupyter to run through the exercise.\n",
        "\n",
        "For a refresher on Python check out this exercise:\n",
        "\n",
        "https://colab.research.google.com/github/cs231n/cs231n.github.io/blob/master/python-colab.ipynb\n",
        "\n",
        "For a specific intro on Colab check out the following exercise:\n",
        "\n",
        "https://colab.research.google.com/notebooks/basic_features_overview.ipynb\n",
        "\n",
        "The main tool we use is a library called Pygimli:\n",
        "\n",
        "https://www.pygimli.org/pygimliapi/index.html\n",
        "\n",
        "Further exercises can be found here:\n",
        "\n",
        "https://www.pygimli.org/_examples_auto/index.html\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Environment setup\n",
        "We need to set up the Python environment before we start. If running the exercise on your local machine the setup would have to be modified. We need to install Anaconda (https://www.anaconda.com/) and use it to install Pygimli. Some of the operations in the setup are a bit advanced and are beyond the scope of this exercise. So just run the following two code cells and move on. This may take a couple of minutes so be patient. If running this Notebook on Google Colab you may get a warning that the Notebook was not authored by Google, click OK and continue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQg3atMq8FeN"
      },
      "source": [
        "%%bash\n",
        "PYTHONPATH=\"\"\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT &> setup.log\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX &>> setup.log\n",
        "echo \"Miniconda installed\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNalxRsk8uvo"
      },
      "source": [
        "!conda install --channel defaults conda python=3.6 --yes &>> setup.log\n",
        "!conda update --channel defaults --all --yes &>> setup.log\n",
        "print(\"Anaconda updated and set up\")\n",
        "import sys\n",
        "sys.path\n",
        "_ = (sys.path.append(\"/usr/local/lib/python3.6/site-packages\"))\n",
        "\n",
        "!conda install pygimli -c gimli -c conda-forge --yes &>> setup.log\n",
        "print(\"Pygimli installed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VN3rlevRiES"
      },
      "source": [
        "# Python modules\n",
        "Here we load the python modules that we will need for the exercise, including Pygimli."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKRdIXJ5__3j"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pygimli as pg\n",
        "import pygimli.meshtools as mt\n",
        "from pygimli.physics.traveltime import TravelTimeManager\n",
        "from pygimli.viewer.mpl import drawMesh\n",
        "pg.utils.units.quants['vel']['cMap'] = 'inferno_r'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmlNH62ERzoY"
      },
      "source": [
        "We start by creating a synthetic model that will represent our \"true model\". We will then create some synthetic data by solving the forward problem (ray tracing) for the true model.\n",
        "\n",
        "# Geometry set up\n",
        "Here we create some meshes that we will use to define the velocity models. The meshes define the geometry of the model but not the seismic velocity. Seismic velocities will be added in the next step.\n",
        "We create a mesh that will serve as our \"true\" model (mesh_for) and a mesh that will serve as a background model (mesh_bg). The true model contains two elliptical anomalies that we will try to recover with the travel time inversion. The background model will be used as the starting model for the inversion.\n",
        "\n",
        "In creating the geometry we already take into account the location of the sources and receivers as we want them to correspond to a node of the mesh.\n",
        "\n",
        "Since this is a land survey we use lots of receivers and a few shots. We add receiver locations on the surface every 50 m and shot locations every 500 m."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yainYyNGAiVo"
      },
      "source": [
        "# Create elliptical anomalies. Youc an change the size and location of the anomalies to customise your model\n",
        "# Anomaly 1:\n",
        "magma = mt.createCircle(pos=[4000.0, -850.], segments=36, radius=[600.0, 500.0], marker=2,\n",
        "                        area=8000.0)\n",
        "# Anomaly 2:\n",
        "pluton = mt.createCircle(pos=[5700.0, -1000.], segments=36, radius=[600.0, 800.0], marker=3,\n",
        "                        area=8000.0)\n",
        "# Define a function to create land topography \n",
        "def surf(x):\n",
        "    y =150 - 40 * np.cos(2 * np.pi * 1/4000 * x) +x*0.01\n",
        "    return y\n",
        "\n",
        "# We define the location of receivers here so that we can give each one a node in the mesh\n",
        "# receivers every 50 m at the surface\n",
        "recx = np.arange(0., 9050.,50.0)\n",
        "recy = surf(recx)\n",
        "# Shots every 500 m at the surface\n",
        "shotx = np.arange(500.0, 9000.0, 500.)\n",
        "shoty = surf(shotx)\n",
        "\n",
        "# Create topography\n",
        "topo = [[x,surf(x)] for x in recx]\n",
        "topo.append([9000,-2500])\n",
        "topo.append([0,-2500])\n",
        "\n",
        "# Create main polygon for the mesh with topography as top surface\n",
        "main = mt.createPolygon(topo,area=8000,isClosed=True, marker=4)\n",
        "plc = main\n",
        "    \n",
        "# Merge polygons into a single mesh for true model and background model\n",
        "# The variable \"quality\" sets the quality of the mesh. A property related to the\n",
        "# accuracy of the solution. \n",
        "# The variable \"area\" sets the maximum area of each cell in the mesh in m^2.\n",
        "mesh_for = mt.createMesh(plc + magma + pluton, quality=32, area=8000)\n",
        "# The background model doesn't include the elliptical anomalies\n",
        "mesh_bg = mt.createMesh(plc, quality=32, area=8000)\n",
        "\n",
        "# Plot meshes\n",
        "fig, ax = plt.subplots(2,1,figsize=(10,8))\n",
        "pg.show(mesh_bg,markers=True, showMesh=True,ax=ax[0])\n",
        "pg.show(mesh_for,markers=True, showMesh=True,ax=ax[1])\n",
        "ax[0].set_title(\"Background mesh\")\n",
        "ax[1].set_title(\"Forward mesh\")\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iN9MYxcSVwd"
      },
      "source": [
        "# Velocity model\n",
        "Here we add velocity information to the meshes.\n",
        "We create a background velocity by \"hanging\" a 1d velocity model from the surface topography. The true model is built by adding the velocity anomalies and a random perturbation to the background model. The velocity information is stored in separate variables from the mesh files. The random field generator used to create a random perturbation is a bit slow so this may take a couple of minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHSlKvV3AoMm"
      },
      "source": [
        "# Remove boundary markers\n",
        "for boundary in mesh_for.boundaries():\n",
        "    boundary.setMarker(0)\n",
        "for boundary in mesh_bg.boundaries():\n",
        "    boundary.setMarker(0)\n",
        "\n",
        "# Initialise lists that will contain the velocity information\n",
        "vel_for = []\n",
        "vel_bg = []\n",
        "# Define background velocity model\n",
        "a = 3500  # Vp value at the surface in m/s\n",
        "b = 1.0   # Vertical Vp gradient in (m/s)/m\n",
        "# Create velocities \"hanging\" from topography\n",
        "for node in mesh_bg.nodes():\n",
        "    vel_bg.append(a+b*(abs(node.y()-surf(node.x()))))    \n",
        "for node in mesh_for.nodes():\n",
        "    vel_for.append(a+b*(abs(node.y()-surf(node.x()))))\n",
        "\n",
        "# Add velocity info to mesh\n",
        "vel_for = pg.meshtools.nodeDataToCellData(mesh_for,np.array(vel_for))\n",
        "vel_bg = pg.meshtools.nodeDataToCellData(mesh_bg,np.array(vel_bg))\n",
        "\n",
        "# Create reference model with forward mesh for plotting calculating velocity anomaly\n",
        "vel_for_ref = []\n",
        "for node in mesh_for.nodes():\n",
        "    vel_for_ref.append(a+b*(abs(node.y()-surf(node.x()))))\n",
        "vel_for_ref = pg.meshtools.nodeDataToCellData(mesh_for,np.array(vel_for_ref))\n",
        "\n",
        "\n",
        "# Add anomalies to velocity model\n",
        "c = 3600  # Vp value inside anomaly 1\n",
        "d = 400   # Vp anomaly inside anomaly 2\n",
        "for i,cell in enumerate(mesh_for.cells()):\n",
        "    if cell.marker() == 2:\n",
        "        vel_for[i] = c\n",
        "    elif cell.marker() == 3:\n",
        "        vel_for[i] = vel_for[i]+d\n",
        "\n",
        "# Add random perturbations to true model. You can change the strength of the random perturbation to see how it affects the inversion.\n",
        "strength = 100\n",
        "random = pg.utils.generateGeostatisticalModel(mesh_for, I=[1000, 500])\n",
        "vel_for = vel_for+random*strength     \n",
        "\n",
        "# Plot background model and true model\n",
        "fig, ax = plt.subplots(2,1,figsize=(10,8))\n",
        "pg.show(mesh_bg, vel_bg, label=\"Vp (m/s)\",ax=ax[0],cMap=pg.cmap('vel'), nLevs=3,cMin=3500,cMax=6200)\n",
        "pg.show(mesh_for, vel_for, label=\"Vp (m/s)\",ax=ax[1],cMap=pg.cmap('vel'), nLevs=3,cMin=3500,cMax=6200)\n",
        "ax[0].set_title(\"Background model\")\n",
        "ax[1].set_title(\"True model\")\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yafZoe8JUmvE"
      },
      "source": [
        "# Experiment set up\n",
        "Here we define the acquisition geometry of the experiment and define the variables that we need to simulate the ray propagation.\n",
        "\n",
        "We create source-receiver pairs that will correspond to the data points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eyo1oRVFBPks"
      },
      "source": [
        "from itertools import product\n",
        "\n",
        "# Define shot locations\n",
        "recs = np.zeros((len(recx), 2)) \n",
        "recs[:, 0] = recx  # x\n",
        "recs[:, 1] = recy\n",
        "\n",
        "# Define receiver locations\n",
        "shots = np.zeros((len(shotx), 2)) \n",
        "shots[:, 0] = shotx  # x\n",
        "shots[:, 1] = shoty\n",
        "\n",
        "# Create labels for the receivers\n",
        "recn = np.arange(len(recx))\n",
        "# Find a label for each receiver that correspond to the shot at that position\n",
        "shotn = []\n",
        "for i in recn:\n",
        "    if recx[i] in shotx:\n",
        "        shotn.append(i)     \n",
        "\n",
        "# Define source-receiver pairs \n",
        "rays = list(product(shotn,recn))\n",
        "\n",
        "# Remove pairs were source and receiver are close to each other\n",
        "# You can modify this to exclude short offsets or long offsets from your inversion\n",
        "remove=[]\n",
        "for i,r in enumerate(rays):\n",
        "    if abs(r[0] - r[1]) < 2 :\n",
        "        remove.append(r)        \n",
        "for r in remove:\n",
        "    rays.remove(r)\n",
        "    \n",
        "# Create empty data container\n",
        "scheme = pg.DataContainer()        \n",
        "\n",
        "# Add receivers and receivers as \"sensors\". Pygimli doesn't distinguish\n",
        "# between sources and receivers. They are all called \"sensors\".\n",
        "for i in recn:\n",
        "    scheme.createSensor((recx[i],recy[i]))\n",
        "\n",
        "# Add measurements, i.e. read the source-receiver pairs defined above\n",
        "rays = np.array(rays)\n",
        "scheme.resize(len(rays))\n",
        "scheme[\"s\"] = rays[:, 0]\n",
        "scheme[\"g\"] = rays[:, 1]\n",
        "scheme[\"valid\"] = np.ones(len(rays))\n",
        "scheme.registerSensorIndex(\"s\")\n",
        "scheme.registerSensorIndex(\"g\")\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRLO9RJzUxXj"
      },
      "source": [
        "# Calculate traveltimes and draw raypaths\n",
        "Here we calculate synthetic traveltimes for the true model. These will be our data for the following inversion so we call them *tobs* (for observed travel times).\n",
        "\n",
        "The traveltimes can be calculated with a range of different method. In this exercise we use the \"graph\" method based on the work of computer scientist Edsger W. Dijkstra. His algorithm was designed to find the shortest path between points on a graph. A \"graph\" in this case is a set of nodes connected by segments, think for example of finding the shortest route from one address to another in central London.\n",
        "\n",
        "The traveltimes calculation uses a modified version of the Dijkstra algorithm that finds the path of minimum traveltime between each source and receiver pair. This is not as simple as finding the minimum distance, because the speed of propagation changes throughout the model so the shortest path is not always the quickest. In our case the network of nodes is the mesh defined above.\n",
        "\n",
        "In this exercise we use the first arrival travel times, i.e. the travel time of the very first seismic wave that reaches the receiver. These are usually the refractions. Refractions are also called transmitted waves or diving waves.\n",
        "\n",
        "**Question 1:** Why is the graph method a good way for calculating the first arrival traveltimes? Are there any disadvantages? How would you modify the method to use relfections?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1Ff0YdVBU-l"
      },
      "source": [
        "# Initialise traveltime manager\n",
        "vel_run = vel_for\n",
        "mesh_run = mesh_for\n",
        "tt = TravelTimeManager()\n",
        "secnodes=5  # Number of secondary nodes\n",
        "\n",
        "pg.tic()\n",
        "tobs = tt.simulate(mesh=mesh_run, scheme=scheme, slowness=1./vel_run,secNodes=secnodes,\n",
        "                 verbose=1,debug=True,noiseAbs=0.0005)\n",
        "pg.toc(\"Raytracing with %d secondary nodes:\" % secnodes)\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5SmcG87VIa7"
      },
      "source": [
        "# Plotting the ray paths\n",
        "Here we plot the paths of the rays calculated in the previous step. Notice how the rays \"avoid\" the low velocity anomaly.\n",
        "\n",
        "**Question 2:** Why are the rays avoiding the low velocity anomaly? Remember that we are calculating the first arrival traveltimes.\n",
        "\n",
        "**Question 3:** What will be the consequences of the rays avoiding the low velocity anomaly on the ability of the inversion to recover the structure?\n",
        "\n",
        "**Question 4:** Notice the maximum depth of penetration of the rays (the turning depth). How could we change the experiment geometry to sample deeper into the subsurface? What else could we do to sample deeper? What factors may make it harder to image deeper?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTDkpcC6BetN"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "# Plot the velocity model\n",
        "pg.show(mesh_run, vel_run, ax=ax, label=\"Velocity (m/s)\", hold=True,\n",
        "       logScale=False, cMap=pg.cmap('vel'), nLevs=3,cMin=3500,cMax=6200)\n",
        "\n",
        "# Plot rays\n",
        "tt.drawRayPaths(ax=ax, model=vel_run, color=\"k\", lw=0.3, alpha=0.5)\n",
        "ax.plot(shotx, shoty, \"rs\", ms=8)     # plot shots as red squares\n",
        "ax.plot(recx, recy, \"bo\", ms=3) # plot receivers as blue dots\n",
        "ax.set_ylim(mesh_run.ymin(), 400)   # set y axis limits\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD_yzR9IVksG"
      },
      "source": [
        "# Ploting the traveltimes\n",
        "Here we plot the synthetic observed traveltimes. Traveltimes for each receiver are plotted in a different color.\n",
        "\n",
        "**Question 5:** What is the signature of the low velocity anomaly in the traveltime curves?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBUO9d2fBh0b"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "ax.set_ylim(0, 2.0)\n",
        "ax.set_xlim(0, 9000)\n",
        "pg.physics.traveltime.drawFirstPicks(ax, tobs)\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pzT8flOVvzK"
      },
      "source": [
        "# Inversion\n",
        "Here we carry out an inversion of the observed traveltimes *tobs* calculated above. The observed traveltimes represent our data. The goal is to match the data and recover the anomalies of the true model. Our starting model is the background velocity model.\n",
        "\n",
        "The inversion is an iterative process. At each iteration we need to trace the rays and calculate the traveltimes so that we can compare them to the data and calculate the traveltime residuals. We then update the model to minimise the traveltime residuals. In practise we minimise the sum of the squares of the residuals weighted by the data uncertainties. Therefore we call this problem a \"Least Squares minimisation\".\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Pitfalls\n",
        "You can see from the ray paths plotted above that the data coverage is irregular and incomplete, this makes the problem of inverting the data \"ill posed\". Because of the irregular and patchy \"illumination\", a wide range of models may exist that all satisfy the data. This set of plausible models is called the null-space. Any noise that exists within the null space will be unconstrained during the inversion and can make the inversion unstable\n",
        "\n",
        "The relationship between the model parameters (the velocity values at the grid nodes) and the traveltimes is also non-linear and this may lead to local minima in the minimisation. In other words the inversion may get stuck and fail to converge to the global minimum of the least squares objective function.\n",
        "\n",
        "To contrast these issues we stabilize the inversion with regularization. The regularization adds a second term to the objective function that we are minimizing at the same time as we minimize the residuals. In this case the regularization term is the roughness of the model, calculated as the second spatial derivative of the velocity distribution. By minimising the regularization term we favour models that have low roughness (i.e. are smoother).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Tuning the inversion\n",
        "The regularization term is weighted by a constant (lambda). A higher lambda favors smoother models. To choose the appropriate value of lambda we study the trade-off curve between the misfit term (the data norm) and the regularization term (the model norm). Because of its shape this trade-off curve is often called the L-curve. To generate the L-curve we would have to run multiple inversions, each with a different value of lambda and we would then plot the data norm vs. the model norm for the final model of each inversion. This would take more time than we have available for this tutorial so I have calculated the L-curve in advance for you. \n",
        "\n",
        "![](https://raw.githubusercontent.com/michpaulatto/AppliedGeophysicsExercises/main/Images/lcurve.png)\n",
        "\n",
        "**Question 6:** Study the L-curve. What is an appropriate value for the regularization strength lambda? Pick a value and then insert it in the cell below and continue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQoDWT6HstYF"
      },
      "source": [
        "# Enter the value of the regularization weight lambda\n",
        "# in the form on the right\n",
        "lam =  10000#@param {type:\"number\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFSOivXGzWqj"
      },
      "source": [
        "# Choosing a final model\n",
        "How do we chose when to stop the inversion?\n",
        "An iterative inversion presents a dilemma: if at each iteration we get closer to the \"true model\", how do we choose when to stop? Here we use three separate criteria:\n",
        "- When the $\\chi^2 $ is 1. The $\\chi^2$ is a measure of how closely the predicted traveltimes fit the data. A $\\chi^2$ of 1 means that we can fit the data to within the given uncertainty.\n",
        "- When the inversion converges. If the improvement at each step is very small we can stop the inversion. We can set this with the parameter dPhi. This represents the minimum improvement in the data norm (the total misfit) and is initially set to 1%. If dPhi goes below this value we stop the inversion.\n",
        "- When we run out of time. If the inversion is not well tuned it may not converge. In this case we stop the inversion after 20 iterations.\n",
        "\n",
        "You can try changing dPhi or maxiter to change the outcome of the inversion (only do this if you have already completed the exercise and have some time to play around)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsY0WpNtBn3W"
      },
      "source": [
        "# Run the Inversion\n",
        "invmodel = tt.invert(tobs, mesh=mesh_bg, startModel=1./vel_bg, secNodes=5,zWeight=0.5, \n",
        "          verbose=1, maxiter=20, lam=lam, dPhi=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kranQm8596p4"
      },
      "source": [
        "# Plot the residuals\n",
        "Here we trace the rays through the final and starting model and compare the travel time residuals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmBMYMeYMoPy"
      },
      "source": [
        "# Initialise traveltime manager\n",
        "ttinv = TravelTimeManager()\n",
        "secnodes=5  # Number of secondary nodes\n",
        "tinv = ttinv.simulate(mesh=mesh_bg, scheme=scheme, slowness=1./invmodel,secNodes=secnodes,\n",
        "                 verbose=1,debug=True)\n",
        "\n",
        "ttbg = TravelTimeManager()\n",
        "secnodes=5  # Number of secondary nodes\n",
        "tbg = ttbg.simulate(mesh=mesh_bg, scheme=scheme, slowness=1./vel_bg,secNodes=secnodes,\n",
        "                 verbose=1,debug=True)\n",
        "# Plot the residuals\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,8), sharex=True, sharey=True)\n",
        "ax.set_title(\"Travel time residuals for station 1\")\n",
        "ax.axes.set_xlabel('Receiver X coordinate (m)')\n",
        "ax.axes.set_ylabel('Traveltime residual (ms)')\n",
        "plt.scatter(tinv('g')[0:178]*50,((tinv('t')-tobs('t'))[0:178])*100,label='Final residuals')\n",
        "plt.scatter(tbg('g')[0:178]*50,((tbg('t')-tobs('t'))[0:178])*100,label='Initial residuals')\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXjSz8kNWBZP"
      },
      "source": [
        "# Plotting the final model\n",
        "Here we plot a comparison of the true model and the recovered model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK5kr7Y3D7ck"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,8), sharex=True, sharey=True)\n",
        "ax1.set_title(\"True model\")\n",
        "ax2.set_title(\"Inversion result\")\n",
        "\n",
        "pg.show(mesh_for, vel_for, ax=ax1, showMesh=False,\n",
        "        label=pg.unit('vel'), cMap=pg.cmap('vel'), nLevs=3,cMin=3500,cMax=6200)\n",
        "\n",
        "tt.showResult(ax=ax2, logScale=False, showMesh=False, nLevs=3,cMin=3500,cMax=6200)\n",
        "\n",
        "#tt.drawRayPaths(ax=ax2, color=\"0.8\", alpha=0.3)\n",
        "for ax in (ax1, ax2):\n",
        "    ax.plot(shotx, shoty, \"rs\", ms=8)\n",
        "    ax.plot(recx, recy, \"ko\", ms=3)\n",
        "    ax.set_ylim(-1600,400)\n",
        "    ax.set_xlim(1000,8000)\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH_88P98YEWE"
      },
      "source": [
        "# Velocity anomaly\n",
        "We can more easily evaluate the results if we plot the true and recovered velocity anomaly, i.e. the difference between the final model and the starting background model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lChzr_G3YRNz"
      },
      "source": [
        "# Subtract the background to get the velocity anomaly\n",
        "ano_true = vel_for - vel_for_ref\n",
        "ano_inv = invmodel - vel_bg\n",
        "\n",
        "# Set up the plot\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,8), sharex=True, sharey=True)\n",
        "ax1.set_title(\"True model anomaly\")\n",
        "ax2.set_title(\"Inversion result anomaly\")\n",
        "\n",
        "# Plot the true anomaly\n",
        "pg.show(mesh_for, ano_true, ax=ax1, showMesh=False,\n",
        "        label=pg.unit('vel'), cMap='seismic', nLevs=3,cMin=-500,cMax=500)\n",
        "\n",
        "# Plot the recovered anomaly\n",
        "pg.show(mesh_bg, ano_inv, ax=ax2, showMesh=False,\n",
        "        label=pg.unit('vel'), cMap='seismic', nLevs=3,cMin=-500,cMax=500)\n",
        "\n",
        "# Plot stations and receivers\n",
        "for ax in (ax1, ax2):\n",
        "    ax.plot(shotx, shoty, \"rs\", ms=8)\n",
        "    ax.plot(recx, recy, \"ko\", ms=3)\n",
        "    ax.set_ylim(-1600,400)\n",
        "    ax.set_xlim(1000,8000)\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X7WPBoBkVF-"
      },
      "source": [
        "# Final remarks\n",
        "The recovered anomalies are considerably weaker than the true anomalies and they are smeared in the horizontal direction.\n",
        "\n",
        "**Question 7:** How could we do better? Consider 1) the experiment parameters, 2) the inversion parameters and 3) the inversion approach.\n",
        "\n",
        "With just the Vp information it would be difficult to interpret the low velocity anomaly with any confidence. When interpretating the results of seismic tomography we always try to incorporate other available data. These are examples of data that can be useful:\n",
        "- Surface geology and tectonic setting;\n",
        "- Location of local seismicity;\n",
        "- Other geophysical data e.g. heat flux or resistivity.\n",
        "\n",
        "If for example the experiment location was in an active volcanic setting we may interpret the low velocity as either i) a zone of partial melt or ii) heated magmatic volatiles. The location of seismicity or thermobarometry constraints from the erupted lavas may then help us rule out one of the two options.\n",
        "\n",
        "If we had carried out a real field experiment and had imaged such a low velocity anomaly we would probably be quite excited. That's how I felt when I saw for the first time the results from the Montserrat tomography experiment described in [Paulatto et al. (2012)](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2011GC003892), that revealed the extent of the magma reservoir beneath SufriÃ©re Hills Volcano.\n",
        "\n",
        "**Question 8:** Suppose you had good evidence indicating that the low velocity anomaly recovered by your inversion was caused by a partially molten magma reservoir. You may then attempt to estimate the melt fraction from the Vp anomaly. How would the limited resolution affect your melt estimate?\n"
      ]
    }
  ]
}